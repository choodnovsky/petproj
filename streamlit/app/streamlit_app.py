import streamlit as st
import json
from sentence_transformers import SentenceTransformer
import chromadb
import ollama


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
context = ""  # –ù–∞—á–∞–ª—å–Ω—ã–π –ø—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç


# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ JSON —Ñ–∞–π–ª–∞
def load_qa_pairs(json_file):
    with open(json_file, "r", encoding="utf-8") as file:
        qa_pairs = json.load(file)
    return qa_pairs


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
def update_context(question, answer):
    global context
    context += f"–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç: {answer}\n---\n"  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å –∏ –æ—Ç–≤–µ—Ç


# –ó–∞–ø—Ä–æ—Å –≤ ChromaDB
def query_chromadb(collection_name, question, top_k=4):
    global context

    client = chromadb.HttpClient(host="localhost", port=8000)
    collection = client.get_collection(collection_name)

    model = SentenceTransformer("all-MiniLM-L6-v2")
    question_embedding = model.encode([question])[0].tolist()

    results = collection.query(
        query_embeddings=[question_embedding],
        n_results=top_k,
        include=["documents", "distances"]
    )

    if not results or "documents" not in results or not results["documents"]:
        return "–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.", []

    documents = results["documents"][0]
    distances = results.get("distances", [None])[0]

    chunks_info = []
    context_parts = []
    for i, doc in enumerate(documents):
        distance = distances[i] if distances else "N/A"
        chunks_info.append(f"üîπ –ß–∞–Ω–∫ #{i+1} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance:.4f}):\n{doc}\n")
        context_parts.append(doc.strip())

    context += "\n".join(context_parts)
    return "\n".join(chunks_info), context_parts


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ —Å —Ü–µ–ø–æ—á–∫–æ–π —Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–π
def query_with_thought_chain(question):
    global context
    prompt = f"""
    –¢—ã ‚Äî –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∫–∞–∫ –º–æ–∂–Ω–æ —Ç–æ—á–Ω–µ–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.
    
    –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
    
    –ö–æ–Ω—Ç–µ–∫—Å—Ç:
    {context}
    –í–æ–ø—Ä–æ—Å:
    {question}
    –û—Ç–≤–µ—Ç (—Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º):
    """

    response = ollama.chat(
        model="llama3",
        messages=[{"role": "user", "content": prompt}]
    )

    answer = response["message"]["content"]
    update_context(question, answer)
    return answer


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –æ—Ç–≤–µ—Ç–∞–º–∏ –∏–∑ JSON
def integrate_qa_pairs(qa_pairs):
    global context
    for pair in qa_pairs:
        question = pair["question"]
        answer = pair["answer"]
        update_context(question, answer)


qa_pairs = load_qa_pairs("./data/qa_pairs.json")
integrate_qa_pairs(qa_pairs)

# Streamlit UI
st.subheader("RAG-–≤–æ–ø—Ä–æ—Å –∫ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ")

collection = st.selectbox("–ö–æ–ª–ª–µ–∫—Ü–∏—è –≤ ChromaDB", options=["wiki_docs", "payments", "reports"])
question = st.text_area("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å")

if st.button("–ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç"):
    with st.spinner("–ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã..."):
        chunks_info, _ = query_chromadb(collection, question)
        # st.markdown("### –ù–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏")
        # st.text(chunks_info)

    with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç..."):
        answer = query_with_thought_chain(question)
        st.markdown("### –û—Ç–≤–µ—Ç")
        st.write(answer)
