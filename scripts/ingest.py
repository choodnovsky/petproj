import argparse
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import os


def clean_text(text: str) -> str:
    lines = text.splitlines()
    return "\n".join([line.strip() for line in lines if line.strip()])


def main(file_path: str):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")

    with open(file_path, "r", encoding="utf-8") as f:
        text = clean_text(f.read())

    print(f"üìñ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {file_path}")
    print(f"üìè –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_text(text)
    print(f"‚úÇÔ∏è  –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤")

    model = SentenceTransformer("all-MiniLM-L6-v2")
    embeddings = model.encode(chunks, show_progress_bar=True)

    client = chromadb.HttpClient(host="localhost", port=8000)
    collection = client.get_or_create_collection("lotr")

    print(f"üì¶ –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏ –≤ ChromaDB...")
    for i, (chunk, emb) in tqdm(enumerate(zip(chunks, embeddings)), total=len(chunks)):
        collection.add(
            documents=[chunk],
            embeddings=[emb.tolist()],
            ids=[f"chunk_{i}"],
            metadatas=[{"source": os.path.basename(file_path), "index": i}]
        )

    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –í–µ–∫—Ç–æ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é 'lotr'")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ ChromaDB")
    parser.add_argument(
        "--file",
        type=str,
        default="../data/lotr.txt",
        help="–ü—É—Ç—å –∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: data/lotr.txt)"
    )
    args = parser.parse_args()
    main(args.file)