import os
import chromadb
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–µ–π
embed_model = SentenceTransformer("all-MiniLM-L6-v2")  # 384-–º–µ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=400)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ChromaDB
client = chromadb.HttpClient(host="localhost", port=8000)
collection_name = "wiki_docs"

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
try:
    collection = client.get_collection(collection_name)
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    if collection and collection.metadata and 'dimension' in collection.metadata:
        expected_dim = collection.metadata["dimension"]
    else:
        expected_dim = None

    actual_dim = embed_model.get_sentence_embedding_dimension()

    if expected_dim and actual_dim != expected_dim:
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ({actual_dim}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ({expected_dim})")
        print("–£–¥–∞–ª—è–µ–º –∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é...")
        client.delete_collection(collection_name)
        collection = client.create_collection(collection_name)
    else:
        print("–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –∏–ª–∏ –∫–æ–ª–ª–µ–∫—Ü–∏—è –µ—â—ë –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

except chromadb.errors.NotFoundError:
    print("–ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ ‚Äî —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é")
    collection = client.create_collection(collection_name)


# --- –§—É–Ω–∫—Ü–∏–∏ ---
def split_text(text):
    return text_splitter.split_text(text)

def add_to_chroma(docs, source_name):
    print(f"–î–æ–±–∞–≤–ª—è–µ–º {len(docs)} —á–∞–Ω–∫–æ–≤ –∏–∑ {source_name}")
    for i, doc in tqdm(enumerate(docs), total=len(docs), desc=f"{source_name}"):
        embedding = embed_model.encode(doc).tolist()
        collection.add(
            documents=[doc],
            embeddings=[embedding],
            metadatas=[{"source": source_name, "index": i}],
            ids=[f"{source_name}_{i}"]
        )

def load_and_index_files(folder_path="../data/wiki/"):
    for filename in os.listdir(folder_path):
        filepath = os.path.join(folder_path, filename)
        if filename.endswith((".txt", ".md")):
            with open(filepath, "r", encoding="utf-8") as f:
                text = f.read()
                chunks = split_text(text)
                add_to_chroma(chunks, filename)
                print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
        else:
            print(f"–ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç): {filename}")


# üîπ –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    load_and_index_files()
    print("–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")