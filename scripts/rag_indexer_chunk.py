import os
import chromadb
import logging
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
from chromadb.errors import NotFoundError

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
CHROMA_HOST = "localhost"
CHROMA_PORT = 8000
COLLECTION_NAME = "wiki_docs"
FOLDER_PATH = "../data/wiki/"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 400

# –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# –ú–æ–¥–µ–ª—å –∏ –∫–ª–∏–µ–Ω—Ç
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE,
    chunk_overlap=CHUNK_OVERLAP
)
client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)


# –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
def get_or_create_collection(name):
    try:
        collection = client.get_collection(name)

        actual_dim = embed_model.get_sentence_embedding_dimension()
        expected_dim = None

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–∞—è: metadata –º–æ–∂–µ—Ç –±—ã—Ç—å None
        if collection.metadata and "dimension" in collection.metadata:
            expected_dim = collection.metadata["dimension"]

        if expected_dim and expected_dim != actual_dim:
            logging.warning(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ ({actual_dim}) ‚â† –∫–æ–ª–ª–µ–∫—Ü–∏–∏ ({expected_dim}). –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º.")
            client.delete_collection(name)
            return client.create_collection(name)
        else:
            return collection

    except NotFoundError:
        logging.info("–ö–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é.")
        return client.create_collection(name)


collection = get_or_create_collection(COLLECTION_NAME)


# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
def split_text(text):
    chunks = text_splitter.split_text(text)
    return [chunk.strip() for chunk in chunks if chunk.strip()]


# –ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª –ª–∏ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª
def is_file_already_indexed(filename):
    try:
        results = collection.get(where={"source": filename}, limit=1)
        return len(results["documents"]) > 0
    except Exception as e:
        logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞ {filename}: {e}")
        return False


# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ ChromaDB
def add_to_chroma(docs, source_name):
    for i, doc in enumerate(docs):
        doc_id = f"{source_name}_{i}"
        embedding = embed_model.encode(doc).tolist()
        collection.add(
            documents=[doc],
            embeddings=[embedding],
            metadatas=[{"source": source_name, "chunk": i}],
            ids=[doc_id]
        )


# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
def load_and_index_files(folder_path):
    files = [f for f in os.listdir(folder_path) if f.endswith((".txt", ".md"))]

    for filename in tqdm(files, desc="üìÑ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤"):
        if is_file_already_indexed(filename):
            logging.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω (—É–∂–µ –≤ –±–∞–∑–µ): {filename}")
            continue

        filepath = os.path.join(folder_path, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as file:
                text = file.read()
                chunks = split_text(text)

                if chunks:
                    add_to_chroma(chunks, filename)
                    logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {filename} ({len(chunks)} —á–∞–Ω–∫–æ–≤)")
                else:
                    logging.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–ª–∏ –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ–∞–π–ª: {filename}")

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {e}")


# –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞
if __name__ == "__main__":
    load_and_index_files(FOLDER_PATH)
    logging.info("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")