## Копроративный информационный помощник

- ### Проблема:
- ___Информационная перегрузка___ — в крупных компаниях документы, инструкции, базы знаний распределены по разным каналам (PDF, Wiki, внутренние порталы). 
- ___Трудности поиска___ — сотрудники тратят значительное время на ручной поиск информации. 
- ___Зависимость от экспертов___ — многие вопросы направляются повторно к одним и тем же специалистам. 
- ___Потери времени при онбординге новых сотрудников___ — необходимость в наставничестве и повторяющихся ответах на одни и те же вопросы.  


- ### Задача: 
- Создание корпоративного интеллектуального помощника, способного оперативно и точно отвечать на вопросы сотрудников, 
основываясь на внутренней документации компании. Система призвана сократить время на поиск информации, 
минимизировать влияние «человеческого фактора» и повысить производительность.

- ### Попутно решаемые задачи:  
- ___HR___ Оценка как новый сотрудник интересуется бизнес-процессами
- ___СБ___ Какими именно бизнес-процессами интересуется новый сотрудник

-------------------

- ### Пошаговое описание проекта:
1. Собираем контейнеры с инфраструктурой  
   - `Ollama` - хранение моделей
   - `CromaDB` - хранение векторной базы
2. Скрипты на `python` будут работать локально. После можно обернуть в отельный контейнер
3. `docker-compose up -d --build` запуск всей инфраструктуры
4. `bash clean.sh`  (оставанливаем и убиваем контейнеры и все что с ними связано, чтобы не занимать ресурсы)
5. ~~Для воссатновления базы в постгрес переходим в контейнер `docker exec -it postgres psql -U postgres -c 'CREATE DATABASE dvdrental;'`~~
6. ~~там восстанавливаем базу `docker exec -it postgres pg_restore -U postgres -d dvdrental --verbose /upload_pg/dvdrental_new.tar`~~
7. Проверяем какие есть модели в контйнере с олламой `docker exec -it ollama ollama list` с первого раза должно быть пусто типо того `{"models":[]} `
8. Загрузим в контейнер модель `docker exec -it ollama ollama pull llama3`
9. Соберать всю корпоративную вики:  
   - Если она в виде HTML/Markdown/Confluence/Notion/Google Docs и т.п. — сначала выгрузить текст.   
   - Сохранить все статьи в один или несколько .txt или .md файлов.  в отдельную дирректорию
10. Пофайлово + почанково загрузить все в `cromaDB` скриптом [rag_indexer_chunk.py](scripts/rag_indexer_chunk.py). Предполагается  
что документы будут докладываться, следовательно скрипт будет запускаться по расписанию 1 раз в час
11. Обернуть файл с формированием запроса в приложение работающее в режиме чата вопрос ответ `streamlit` [streamlit_app.py](streamlit/app/streamlit_app.py)
12. У пользователя есть возможность вести диалог с моделью. Если ответ очевидно не корректный можно дать подсказку,  
таким образом улучшать контекст для следующих ответов